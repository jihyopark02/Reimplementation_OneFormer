{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from coco_dataset import COCOPanopticDataset\n",
    "from pixeldecoder import PixelDecoder\n",
    "from backbone import BackboneWithMultiScaleFeatures\n",
    "from tokenizer import TaskTokenizer\n",
    "from mlp import TaskMLP\n",
    "from text_mapper import TextMapper\n",
    "from contrastive_loss import ContrastiveLoss\n",
    "from query_formulation import TaskConditionedQueryFormulator\n",
    "from compute_loss import SetCriterion\n",
    "from hungarian_matcher import HungarianMatcher\n",
    "from transformer_decoder import TransformerDecoder\n",
    "from predict import MaskClassPredictor\n",
    "from torchmetrics import JaccardIndex\n",
    "import pandas as pd\n",
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = 30000\n",
    "embed_dim = 256\n",
    "max_seq_len = 128\n",
    "num_queries = 100\n",
    "temperature = 0.2\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_classes = 80\n",
    "contrastive_weight = 0.5\n",
    "primary_loss_weight = 1.0\n",
    "\n",
    "# COCO Dataset Paths (Change the path if necessary)\n",
    "train_image_dir = \"datasets/coco/train2017\"\n",
    "train_instance_file = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "train_panoptic_file = \"datasets/coco/annotations/panoptic_train2017.json\"\n",
    "train_panoptic_mask_dir = \"datasets/coco/panoptic_train2017\"\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize the dataset and DataLoader\n",
    "full_train_dataset = COCOPanopticDataset(\n",
    "    image_dir=train_image_dir,\n",
    "    instance_file=train_instance_file,\n",
    "    panoptic_file=train_panoptic_file,\n",
    "    panoptic_mask_dir=train_panoptic_mask_dir,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_train_dataset, range(5000))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Contrastive Loss: 5.8376078605651855\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 1 - Contrastive Loss: 5.8376078605651855, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n",
      "Epoch 2/10\n",
      "Contrastive Loss: 5.774187088012695\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 2 - Contrastive Loss: 5.774187088012695, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n",
      "Epoch 3/10\n",
      "Contrastive Loss: 5.71146821975708\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 3 - Contrastive Loss: 5.71146821975708, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n",
      "Epoch 4/10\n",
      "Contrastive Loss: 5.649392604827881\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 4 - Contrastive Loss: 5.649392604827881, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n",
      "Epoch 5/10\n",
      "Contrastive Loss: 5.58760929107666\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 5 - Contrastive Loss: 5.58760929107666, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n",
      "Epoch 6/10\n",
      "Contrastive Loss: 5.526155948638916\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 6 - Contrastive Loss: 5.526155948638916, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n",
      "Epoch 7/10\n",
      "Contrastive Loss: 5.464902400970459\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 7 - Contrastive Loss: 5.464902400970459, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n",
      "Epoch 8/10\n",
      "Contrastive Loss: 5.403698921203613\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 8 - Contrastive Loss: 5.403698921203613, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n",
      "Epoch 9/10\n",
      "Contrastive Loss: 5.342477798461914\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 9 - Contrastive Loss: 5.342477798461914, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n",
      "Epoch 10/10\n",
      "Contrastive Loss: 5.281089782714844\n",
      "Cross-entropy loss: 0.0\n",
      "Epoch 10 - Contrastive Loss: 5.281089782714844, Cross-Entropy Loss: 0.0, mIoU: 9.5367431640625e-06\n"
     ]
    }
   ],
   "source": [
    "backbone = BackboneWithMultiScaleFeatures()\n",
    "pixel_decoder = PixelDecoder(input_channels=[256, 512, 1024, 2048])\n",
    "tokenizer = TaskTokenizer(vocab_size, embed_dim, max_seq_len)\n",
    "mlp = TaskMLP(input_dim=embed_dim, hidden_dim=embed_dim, output_dim=embed_dim)\n",
    "text_mapper = TextMapper(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "contrastive_loss_fn = ContrastiveLoss(temperature)\n",
    "task_query_formulator = TaskConditionedQueryFormulator(num_queries=num_queries, embed_dim=embed_dim)\n",
    "matcher = HungarianMatcher(cost_class=1, cost_mask=1, cost_dice=1)\n",
    "criterion = SetCriterion(matcher=matcher, num_classes=num_classes, weight_dict={'loss_ce': 1, 'loss_mask': 1, 'loss_dice': 1}, eos_coef=0.1, losses=['labels', 'masks'])\n",
    "transformer_decoder = TransformerDecoder(\n",
    "    embed_dim=embed_dim,\n",
    "    num_queries=num_queries,\n",
    "    num_classes=num_classes,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "mask_class_predictor = MaskClassPredictor(embed_dim, num_queries, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": backbone.parameters()},\n",
    "    {\"params\": pixel_decoder.parameters()},\n",
    "    {\"params\": transformer_decoder.parameters()},\n",
    "    {\"params\": mask_class_predictor.parameters()},\n",
    "    {\"params\": mlp.parameters()},\n",
    "    {\"params\": text_mapper.parameters()},\n",
    "    {\"params\": task_query_formulator.parameters()},\n",
    "], lr=1e-4)\n",
    "\n",
    "miou_metric = JaccardIndex(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "all_pred_logits, all_pred_masks, all_gt_labels, all_gt_masks = [], [], [], []\n",
    "\n",
    "# This is for generating graphs\n",
    "epoch_list = []\n",
    "contrastive_loss_list = []\n",
    "cross_entropy_loss_list = []\n",
    "miou_list = []\n",
    "\n",
    "num_epochs = 10 # You can change this number, but more than 10 might take a long time.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    epoch_contrastive_loss = 0.0\n",
    "    epoch_cross_entropy_loss = 0.0\n",
    "    epoch_miou = 0.0\n",
    "\n",
    "    for image_batch, mask_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        multi_scale_features = backbone(image_batch)\n",
    "        decoded_features = pixel_decoder(multi_scale_features)\n",
    "        image_features_1_4 = decoded_features[0]\n",
    "\n",
    "        task_texts = [\"panoptic\", \"instance\", \"semantic\"]\n",
    "        task_embeddings = tokenizer.forward(task_texts)  # [3, max_seq_len, embed_dim]\n",
    "        task_embeddings = mlp(task_embeddings.mean(dim=1).unsqueeze(1)).squeeze(1)  # [3, embed_dim]\n",
    "\n",
    "        q_text = text_mapper(\n",
    "            panoptic_text=task_embeddings[0].unsqueeze(0).long(),\n",
    "            instance_text=task_embeddings[1].unsqueeze(0).long(),\n",
    "            semantic_text=task_embeddings[2].unsqueeze(0).long()\n",
    "        )\n",
    "\n",
    "        #Generate Q_task\n",
    "        batch_size = image_batch.size(0)\n",
    "        q_task = task_query_formulator(task_embeddings.unsqueeze(1), batch_size).permute(1, 0, 2)\n",
    "\n",
    "        contrastive_loss = contrastive_loss_fn(q_text, q_task)\n",
    "        print(f\"Contrastive Loss: {contrastive_loss.item()}\")\n",
    "\n",
    "        if q_text.size(0) == 1:\n",
    "            q_text = q_text.expand(q_task.size(0), -1, -1)\n",
    "\n",
    "        q_text = F.normalize(q_text, dim=-1)\n",
    "        q_task = F.normalize(q_task, dim=-1)\n",
    "\n",
    "        batch_size, num_tasks, embed_dim = q_text.size()\n",
    "        _, num_queries, _ = q_task.size()\n",
    "        \n",
    "        q_text = q_text.reshape(batch_size * num_tasks, embed_dim)\n",
    "        q_task = q_task.reshape(batch_size * num_queries, embed_dim)\n",
    "\n",
    "        decoder_output = transformer_decoder(q_task, multi_scale_features)\n",
    "\n",
    "        flattened_image_features_1_4 = image_features_1_4.view(1, embed_dim, 128 * 128).permute(0, 2, 1)\n",
    "\n",
    "        combined_input = torch.cat([decoder_output, flattened_image_features_1_4], dim=1)\n",
    "\n",
    "        mask_pred, class_pred = mask_class_predictor(combined_input)\n",
    "        \n",
    "        outputs = {'pred_logits': class_pred, 'pred_masks': mask_pred}\n",
    "        \n",
    "        targets = [{'labels': mask_batch[0]}]\n",
    "        primary_loss = criterion(outputs, targets)\n",
    "        \n",
    "        total_loss = contrastive_weight * contrastive_loss + primary_loss_weight * sum(primary_loss.values())\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(f\"Contrastive Loss: {contrastive_loss.item()}, Primary Loss: {sum(primary_loss.values()).item()}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "        epoch_contrastive_loss += contrastive_loss.item()\n",
    "        epoch_cross_entropy_loss += primary_loss.get('loss_ce', torch.tensor(0.0)).item()\n",
    "\n",
    "        all_pred_logits.append(class_pred)\n",
    "        all_pred_masks.append(mask_pred)\n",
    "        all_gt_labels.append(mask_batch[0])\n",
    "        all_gt_masks.append(mask_batch[0])\n",
    "\n",
    "        mask_pred_resized = F.interpolate(mask_pred, size=(512, 512), mode=\"nearest\")\n",
    "        mask_pred_classes = mask_pred_resized.argmax(dim=1)\n",
    "\n",
    "        mask_pred_classes = mask_pred_classes.squeeze(0).long()\n",
    "        mask_batch_labels = mask_batch[0].long()\n",
    "\n",
    "        mask_batch_labels = mask_batch_labels.clone()\n",
    "        mask_batch_labels[(mask_batch_labels >= num_classes) | (mask_batch_labels < 0)] = -1\n",
    "\n",
    "        mask_pred_classes = mask_pred_classes.to(mask_batch_labels.device)\n",
    "        #mask_pred_classes[(mask_pred_classes >= num_classes) | (mask_pred_classes < 0)] = -1\n",
    "        mask_pred_classes = mask_pred_classes.clamp(0, num_classes - 1)\n",
    "        mask_batch_labels = mask_batch_labels.clamp(0, num_classes - 1)\n",
    "        \n",
    "        mask_pred_classes = mask_pred_classes.view(-1)\n",
    "        mask_batch_labels = mask_batch_labels.view(-1)\n",
    "\n",
    "        mask_pred_classes[mask_pred_classes < 0] = -1\n",
    "        mask_batch_labels[mask_batch_labels < 0] = -1\n",
    "\n",
    "        miou_metric.update(mask_pred_classes, mask_batch_labels)\n",
    "\n",
    "        break\n",
    "\n",
    "    epoch_miou = miou_metric.compute().item()\n",
    "    miou_metric.reset()\n",
    "\n",
    "    # Append metrics for this epoch to lists\n",
    "    epoch_list.append(epoch + 1)\n",
    "    contrastive_loss_list.append(epoch_contrastive_loss)\n",
    "    cross_entropy_loss_list.append(epoch_cross_entropy_loss)\n",
    "    miou_list.append(epoch_miou)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Contrastive Loss: {epoch_contrastive_loss}, Cross-Entropy Loss: {epoch_cross_entropy_loss}, mIoU: {epoch_miou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to training_metrics.csv\n",
      "   Epoch  Contrastive Loss  Cross-Entropy Loss     mIoU\n",
      "0      1          5.837608                 0.0  0.00001\n",
      "1      2          5.774187                 0.0  0.00001\n",
      "2      3          5.711468                 0.0  0.00001\n",
      "3      4          5.649393                 0.0  0.00001\n",
      "4      5          5.587609                 0.0  0.00001\n",
      "5      6          5.526156                 0.0  0.00001\n",
      "6      7          5.464902                 0.0  0.00001\n",
      "7      8          5.403699                 0.0  0.00001\n",
      "8      9          5.342478                 0.0  0.00001\n",
      "9     10          5.281090                 0.0  0.00001\n"
     ]
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    \"Epoch\": epoch_list,\n",
    "    \"Contrastive Loss\": contrastive_loss_list,\n",
    "    \"Cross-Entropy Loss\": cross_entropy_loss_list,\n",
    "    \"mIoU\": miou_list\n",
    "})\n",
    "\n",
    "# Save the metrics DataFrame to CSV\n",
    "metrics_df.to_csv(\"training_metrics.csv\", index=False)\n",
    "print(\"Metrics saved to training_metrics.csv\")\n",
    "\n",
    "# Display the DataFrame for verification\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
