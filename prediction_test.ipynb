{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from coco_dataset import COCOPanopticDataset\n",
    "from load_data import train_loader\n",
    "from pixeldecoder import PixelDecoder\n",
    "from backbone import BackboneWithMultiScaleFeatures\n",
    "from tokenizer import TaskTokenizer\n",
    "from mlp import TaskMLP\n",
    "from text_mapper import TextMapper\n",
    "from contrastive_loss import ContrastiveLoss\n",
    "from query_formulation import TaskConditionedQueryFormulator\n",
    "from compute_loss import SetCriterion\n",
    "from hungarian_matcher import HungarianMatcher\n",
    "from transformer_decoder import TransformerDecoder\n",
    "from predict import MaskClassPredictor\n",
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = 30000\n",
    "embed_dim = 256\n",
    "max_seq_len = 128\n",
    "num_queries = 100\n",
    "temperature = 0.07\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_classes = 80\n",
    "\n",
    "# COCO Dataset Paths\n",
    "train_image_dir = \"datasets/coco/train2017\"\n",
    "train_instance_file = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "train_panoptic_file = \"datasets/coco/annotations/panoptic_train2017.json\"\n",
    "train_panoptic_mask_dir = \"datasets/coco/panoptic_train2017\"\n",
    "\n",
    "# Define transformation for images and masks\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize the dataset and DataLoader\n",
    "full_train_dataset = COCOPanopticDataset(\n",
    "    image_dir=train_image_dir,\n",
    "    instance_file=train_instance_file,\n",
    "    panoptic_file=train_panoptic_file,\n",
    "    panoptic_mask_dir=train_panoptic_mask_dir,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_train_dataset, range(5000))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panoptic mask 175611.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 150235.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 30156.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 359959.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 325027.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 180800.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 469982.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 548331.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 482829.png not found in datasets/coco/panoptic_train2017\n",
      "Input shape before reshaping: torch.Size([3, 1, 256])\n",
      "Shape after flattening for MLP: torch.Size([3, 256])\n",
      "Shape after MLP processing: torch.Size([3, 256])\n",
      "Shape after reshaping back to [batch_size, seq_len, output_dim]: torch.Size([3, 1, 256])\n",
      "Input shape before reshaping: torch.Size([3, 1, 256])\n",
      "Shape after flattening for MLP: torch.Size([3, 256])\n",
      "Shape after MLP processing: torch.Size([3, 256])\n",
      "Shape after reshaping back to [batch_size, seq_len, output_dim]: torch.Size([3, 1, 256])\n",
      "3\n",
      "Contrastive Loss: inf\n",
      "Shape of q_task: torch.Size([3, 100, 256])\n",
      "Shape of flattened_features: torch.Size([3, 262144, 256])\n",
      "Shape of combined_input before TransformerDecoder: torch.Size([3, 262244, 256])\n",
      "Shape of q_task: torch.Size([3, 1, 100, 256])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of q_task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_task\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Step 7: Pass through Transformer Decoder\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_task\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Step 8: Mask and Class Prediction\u001b[39;00m\n\u001b[1;32m     67\u001b[0m mask_pred, class_pred \u001b[38;5;241m=\u001b[39m mask_class_predictor(decoder_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Reimplementation_OneFormer/transformer_decoder.py:22\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, multi_scale_features, task_queries)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, multi_scale_features, task_queries):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Step 1: Flatten and Concatenate Multi-Scale Features for Transformer Input\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[0;32m---> 22\u001b[0m         \u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmulti_scale_features\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m     )  \u001b[38;5;66;03m# Shape: [batch_size, total_spatial_dim, embed_dim]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Step 2: Decode Task-Conditioned Queries Using Transformer Layers\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     repeated_queries \u001b[38;5;241m=\u001b[39m task_queries\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, combined_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, total_spatial_dim, num_queries, embed_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Reimplementation_OneFormer/transformer_decoder.py:22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, multi_scale_features, task_queries):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Step 1: Flatten and Concatenate Multi-Scale Features for Transformer Input\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[0;32m---> 22\u001b[0m         [\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m multi_scale_features], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m     )  \u001b[38;5;66;03m# Shape: [batch_size, total_spatial_dim, embed_dim]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Step 2: Decode Task-Conditioned Queries Using Transformer Layers\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     repeated_queries \u001b[38;5;241m=\u001b[39m task_queries\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, combined_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, total_spatial_dim, num_queries, embed_dim]\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "# Initialize Model Components\n",
    "backbone = BackboneWithMultiScaleFeatures()\n",
    "pixel_decoder = PixelDecoder(input_channels=[256, 512, 1024, 2048])\n",
    "tokenizer = TaskTokenizer(vocab_size, embed_dim, max_seq_len)\n",
    "mlp = TaskMLP(input_dim=embed_dim, hidden_dim=embed_dim, output_dim=embed_dim)\n",
    "text_mapper = TextMapper(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "contrastive_loss_fn = ContrastiveLoss(temperature)\n",
    "task_query_formulator = TaskConditionedQueryFormulator(num_queries=num_queries, embed_dim=embed_dim)\n",
    "matcher = HungarianMatcher(cost_class=1, cost_mask=1, cost_dice=1)\n",
    "criterion = SetCriterion(matcher=matcher, num_classes=num_classes, weight_dict={'loss_ce': 1, 'loss_mask': 1, 'loss_dice': 1}, eos_coef=0.1, losses=['labels', 'masks'])\n",
    "transformer_decoder = TransformerDecoder(\n",
    "    embed_dim=embed_dim,\n",
    "    num_queries=num_queries,\n",
    "    num_classes=num_classes,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "mask_class_predictor = MaskClassPredictor(embed_dim, num_queries, num_classes)\n",
    "\n",
    "# Main Training Loop (Single Batch for Debugging)\n",
    "for image_batch, mask_batch in train_loader:\n",
    "    # Step 1: Extract Multi-Scale Features\n",
    "    multi_scale_features = backbone(image_batch)\n",
    "    decoded_features = pixel_decoder(multi_scale_features)\n",
    "    image_features_1_4 = decoded_features[0]  # Select 1/4 resolution features\n",
    "    \n",
    "    # Step 2: Tokenize Task Texts\n",
    "    task_texts = [\"panoptic\", \"instance\", \"semantic\"]\n",
    "    task_embeddings = tokenizer.forward(task_texts)  # [3, max_seq_len, embed_dim]\n",
    "    task_embeddings = mlp(task_embeddings.mean(dim=1).unsqueeze(1)).squeeze(1)  # [3, embed_dim]\n",
    "    \n",
    "    # Step 3: Map Task Embeddings to Q_text\n",
    "    q_text = text_mapper(\n",
    "        panoptic_text=task_embeddings[0].unsqueeze(0).long(),\n",
    "        instance_text=task_embeddings[1].unsqueeze(0).long(),\n",
    "        semantic_text=task_embeddings[2].unsqueeze(0).long()\n",
    "    )\n",
    "    \n",
    "    # Step 4: Generate Q_task\n",
    "    batch_size = image_batch.size(0)\n",
    "    q_task = task_query_formulator(task_embeddings.unsqueeze(1), batch_size).permute(1, 0, 2)\n",
    "    \n",
    "    print(q_task.dim())\n",
    "    # Step 5: Calculate Contrastive Loss between Q_text and Q_task\n",
    "    contrastive_loss = contrastive_loss_fn(q_text)\n",
    "    print(f\"Contrastive Loss: {contrastive_loss.item()}\")\n",
    "    \n",
    "    # Step 6: Flatten and Integrate Image Features\n",
    "    flattened_features = F.interpolate(image_features_1_4, scale_factor=4, mode=\"nearest\")\n",
    "    flattened_features = flattened_features.view(batch_size, embed_dim, -1).permute(0, 2, 1)\n",
    "    flattened_features = flattened_features.repeat(q_task.size(0), 1, 1)  # Corrected here\n",
    "\n",
    "    combined_input = torch.cat([q_task, flattened_features], dim=1)  # No permutation here\n",
    "        # Debugging: Print shapes to ensure correctness\n",
    "    print(f\"Shape of q_task: {q_task.shape}\")\n",
    "    print(f\"Shape of flattened_features: {flattened_features.shape}\")\n",
    "    print(f\"Shape of combined_input before TransformerDecoder: {combined_input.shape}\")\n",
    "\n",
    "    # Make sure q_task and combined_input have compatible dimensions for TransformerDecoder\n",
    "    q_task = q_task.unsqueeze(1) if q_task.dim() == 3 else q_task\n",
    "    print(f\"Shape of q_task: {q_task.shape}\")\n",
    "    # Step 7: Pass through Transformer Decoder\n",
    "    decoder_output = transformer_decoder(combined_input, task_queries=q_task)\n",
    "    \n",
    "    # Step 8: Mask and Class Prediction\n",
    "    mask_pred, class_pred = mask_class_predictor(decoder_output)\n",
    "    \n",
    "    # Step 9: Calculate SetCriterion Loss\n",
    "    outputs = {'pred_logits': class_pred, 'pred_masks': mask_pred}\n",
    "    targets = [{'labels': mask_batch[i]} for i in range(batch_size)]\n",
    "    indices = matcher(outputs, targets)\n",
    "    losses = criterion(outputs, targets)\n",
    "    \n",
    "    print(f\"Losses: {losses}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
