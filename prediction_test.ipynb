{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from coco_dataset import COCOPanopticDataset\n",
    "from pixeldecoder import PixelDecoder\n",
    "from backbone import BackboneWithMultiScaleFeatures\n",
    "from tokenizer import TaskTokenizer\n",
    "from mlp import TaskMLP\n",
    "from text_mapper import TextMapper\n",
    "from contrastive_loss import ContrastiveLoss\n",
    "from query_formulation import TaskConditionedQueryFormulator\n",
    "from compute_loss import SetCriterion\n",
    "from hungarian_matcher import HungarianMatcher\n",
    "from transformer_decoder import TransformerDecoder\n",
    "from predict import MaskClassPredictor\n",
    "from torchmetrics import JaccardIndex\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from panopticapi.evaluation import pq_compute\n",
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = 30000\n",
    "embed_dim = 256\n",
    "max_seq_len = 128\n",
    "num_queries = 100\n",
    "temperature = 0.2\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_classes = 80\n",
    "contrastive_weight = 0.5\n",
    "primary_loss_weight = 1.0\n",
    "\n",
    "# COCO Dataset Paths (Change the path if necessary)\n",
    "train_image_dir = \"datasets/coco/train2017\"\n",
    "train_instance_file = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "train_panoptic_file = \"datasets/coco/annotations/panoptic_train2017.json\"\n",
    "train_panoptic_mask_dir = \"datasets/coco/panoptic_train2017\"\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize the dataset and DataLoader\n",
    "full_train_dataset = COCOPanopticDataset(\n",
    "    image_dir=train_image_dir,\n",
    "    instance_file=train_instance_file,\n",
    "    panoptic_file=train_panoptic_file,\n",
    "    panoptic_mask_dir=train_panoptic_mask_dir,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_train_dataset, range(5000))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Loss: 5.762584686279297\n",
      "Cross-entropy loss: 0.0\n",
      "Contrastive Loss: 5.762584686279297, Primary Loss: 0.0, Total Loss: 2.8812923431396484\n",
      "mask_batch shape: torch.Size([1, 512, 512])\n",
      "mask_pred_classes shape after processing: torch.Size([512, 512])\n",
      "mask_batch_labels shape: torch.Size([512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x11dc7c0e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1568, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "backbone = BackboneWithMultiScaleFeatures()\n",
    "pixel_decoder = PixelDecoder(input_channels=[256, 512, 1024, 2048])\n",
    "tokenizer = TaskTokenizer(vocab_size, embed_dim, max_seq_len)\n",
    "mlp = TaskMLP(input_dim=embed_dim, hidden_dim=embed_dim, output_dim=embed_dim)\n",
    "text_mapper = TextMapper(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "contrastive_loss_fn = ContrastiveLoss(temperature)\n",
    "task_query_formulator = TaskConditionedQueryFormulator(num_queries=num_queries, embed_dim=embed_dim)\n",
    "matcher = HungarianMatcher(cost_class=1, cost_mask=1, cost_dice=1)\n",
    "criterion = SetCriterion(matcher=matcher, num_classes=num_classes, weight_dict={'loss_ce': 1, 'loss_mask': 1, 'loss_dice': 1}, eos_coef=0.1, losses=['labels', 'masks'])\n",
    "transformer_decoder = TransformerDecoder(\n",
    "    embed_dim=embed_dim,\n",
    "    num_queries=num_queries,\n",
    "    num_classes=num_classes,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "mask_class_predictor = MaskClassPredictor(embed_dim, num_queries, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": backbone.parameters()},\n",
    "    {\"params\": pixel_decoder.parameters()},\n",
    "    {\"params\": transformer_decoder.parameters()},\n",
    "    {\"params\": mask_class_predictor.parameters()},\n",
    "    {\"params\": mlp.parameters()},\n",
    "    {\"params\": text_mapper.parameters()},\n",
    "    {\"params\": task_query_formulator.parameters()},\n",
    "], lr=1e-4)\n",
    "\n",
    "# Initialize IoU metric for semantic segmentation\n",
    "miou_metric = JaccardIndex(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "all_pred_logits, all_pred_masks, all_gt_labels, all_gt_masks = [], [], [], []\n",
    "\n",
    "for image_batch, mask_batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #Extract Multi-Scale Features\n",
    "    multi_scale_features = backbone(image_batch)\n",
    "    decoded_features = pixel_decoder(multi_scale_features)\n",
    "    image_features_1_4 = decoded_features[0]\n",
    "    \n",
    "    #Tokenize Task Texts\n",
    "    task_texts = [\"panoptic\", \"instance\", \"semantic\"]\n",
    "    task_embeddings = tokenizer.forward(task_texts)  # [3, max_seq_len, embed_dim]\n",
    "    task_embeddings = mlp(task_embeddings.mean(dim=1).unsqueeze(1)).squeeze(1)  # [3, embed_dim]\n",
    "    \n",
    "    #Map Task Embeddings to Q_text\n",
    "    q_text = text_mapper(\n",
    "        panoptic_text=task_embeddings[0].unsqueeze(0).long(),\n",
    "        instance_text=task_embeddings[1].unsqueeze(0).long(),\n",
    "        semantic_text=task_embeddings[2].unsqueeze(0).long()\n",
    "    )\n",
    "\n",
    "    #Generate Q_task\n",
    "    batch_size = image_batch.size(0)\n",
    "    q_task = task_query_formulator(task_embeddings.unsqueeze(1), batch_size).permute(1, 0, 2)\n",
    "\n",
    "    #Calculate Contrastive Loss between Q_text and Q_task\n",
    "    contrastive_loss = contrastive_loss_fn(q_text, q_task)\n",
    "    print(f\"Contrastive Loss: {contrastive_loss.item()}\")\n",
    "\n",
    "    if q_text.size(0) == 1:\n",
    "        q_text = q_text.expand(q_task.size(0), -1, -1)\n",
    "\n",
    "    q_text = F.normalize(q_text, dim=-1)\n",
    "    q_task = F.normalize(q_task, dim=-1)\n",
    "\n",
    "    batch_size, num_tasks, embed_dim = q_text.size()\n",
    "    _, num_queries, _ = q_task.size()\n",
    "    \n",
    "    q_text = q_text.reshape(batch_size * num_tasks, embed_dim)\n",
    "    q_task = q_task.reshape(batch_size * num_queries, embed_dim)\n",
    "\n",
    "    decoder_output = transformer_decoder(q_task, multi_scale_features)\n",
    "\n",
    "    flattened_image_features_1_4 = image_features_1_4.view(1, embed_dim, 128 * 128).permute(0, 2, 1)\n",
    "\n",
    "    combined_input = torch.cat([decoder_output, flattened_image_features_1_4], dim=1)\n",
    "    \n",
    "    mask_pred, class_pred = mask_class_predictor(combined_input)\n",
    "    \n",
    "    outputs = {'pred_logits': class_pred, 'pred_masks': mask_pred}\n",
    "    targets = [{'labels': mask_batch[0]}]\n",
    "    primary_loss = criterion(outputs, targets)\n",
    "    \n",
    "    total_loss = contrastive_weight * contrastive_loss + primary_loss_weight * sum(primary_loss.values())\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Contrastive Loss: {contrastive_loss.item()}, Primary Loss: {sum(primary_loss.values()).item()}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "    all_pred_logits.append(class_pred)\n",
    "    all_pred_masks.append(mask_pred)\n",
    "    all_gt_labels.append(mask_batch[0])\n",
    "    all_gt_masks.append(mask_batch[0])\n",
    "\n",
    "    mask_pred_resized = F.interpolate(mask_pred, size=(512, 512), mode=\"nearest\")\n",
    "    mask_pred_classes = mask_pred_resized.argmax(dim=1)\n",
    "\n",
    "    mask_pred_classes = mask_pred_classes.squeeze(0).long()\n",
    "    mask_batch_labels = mask_batch[0].long()\n",
    "\n",
    "    mask_batch_labels = mask_batch_labels.clone()\n",
    "    mask_batch_labels[(mask_batch_labels >= num_classes) | (mask_batch_labels < 0)] = -1\n",
    "\n",
    "    mask_pred_classes = mask_pred_classes.to(mask_batch_labels.device)\n",
    "\n",
    "    miou_metric.update(mask_pred_classes, mask_batch_labels)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original mask shape: (1, 100, 128, 128)\n",
      "All compatible masks processed and saved.\n",
      "Mean IoU (mIoU): 3.1534666049992666e-05\n",
      "loading annotations into memory...\n",
      "Done (t=0.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Warning: No image ID found for 000000000000.jpg. Skipping.\n",
      "Error: coco_results is empty. No predictions available for COCO evaluation.\n",
      "Evaluation panoptic segmentation metrics:\n",
      "Ground truth:\n",
      "\tSegmentation folder: datasets/coco/panoptic_val2017\n",
      "\tJSON file: datasets/coco/annotations/panoptic_val2017.json\n",
      "Prediction:\n",
      "\tSegmentation folder: predicted_masks_folder\n",
      "\tJSON file: predictions.json\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: coco_results is empty. No predictions available for COCO evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Panoptic Quality Evaluation\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m pq_results \u001b[38;5;241m=\u001b[39m \u001b[43mpq_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_json_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_panoptic_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_json_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredictions.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/coco/panoptic_val2017\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_folder\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPanoptic Quality (PQ): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpq_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentation Quality (SQ): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpq_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/panopticapi/evaluation.py:211\u001b[0m, in \u001b[0;36mpq_compute\u001b[0;34m(gt_json_file, pred_json_file, gt_folder, pred_folder)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pred_folder):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFolder \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with predicted segmentations doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pred_folder))\n\u001b[0;32m--> 211\u001b[0m pred_annotations \u001b[38;5;241m=\u001b[39m {el[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: el \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpred_json\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mannotations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m}\n\u001b[1;32m    212\u001b[0m matched_annotations_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gt_ann \u001b[38;5;129;01min\u001b[39;00m gt_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image  # Use PIL for saving images\n",
    "import os\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as maskUtils\n",
    "import torch\n",
    "from panopticapi.evaluation import pq_compute\n",
    "\n",
    "# Directory to save predicted masks\n",
    "pred_folder = \"predicted_masks_folder\"\n",
    "os.makedirs(pred_folder, exist_ok=True)\n",
    "\n",
    "# Convert all_pred_masks to images and save them\n",
    "for i, mask in enumerate(all_pred_masks):\n",
    "    mask_np = mask.cpu().detach().numpy()  # (1, num_classes, 128, 128)\n",
    "    print(f\"Original mask shape: {mask_np.shape}\")\n",
    "\n",
    "    if mask_np.shape[1] > 1:\n",
    "        mask_np = np.argmax(mask_np, axis=1)\n",
    "    else:\n",
    "        mask_np = mask_np.squeeze(1)\n",
    "    mask_np = np.squeeze(mask_np)\n",
    "\n",
    "    if mask_np.ndim != 2:\n",
    "        print(f\"Skipping mask {i} due to incompatible shape: {mask_np.shape}\")\n",
    "        continue\n",
    "\n",
    "    mask_np = (mask_np * (255 // (mask_np.max() + 1))).astype(np.uint8)\n",
    "    mask_image = Image.fromarray(mask_np)\n",
    "    mask_filename = os.path.join(pred_folder, f\"pred_mask_{i}.png\")\n",
    "    mask_image.save(mask_filename)\n",
    "\n",
    "print(\"All compatible masks processed and saved.\")\n",
    "\n",
    "# Mean IoU Calculation\n",
    "miou = miou_metric.compute()\n",
    "print(f\"Mean IoU (mIoU): {miou}\")\n",
    "\n",
    "# COCO Evaluation for Instance Segmentation\n",
    "val_instance_file = \"datasets/coco/annotations/instances_val2017.json\"\n",
    "val_panoptic_file = \"datasets/coco/annotations/panoptic_val2017.json\"\n",
    "coco_gt = COCO(val_instance_file)\n",
    "\n",
    "with open(val_panoptic_file, \"r\") as f:\n",
    "    gt_data = json.load(f)\n",
    "image_id_map = {img['file_name']: img['id'] for img in gt_data['images']}\n",
    "\n",
    "coco_results = []\n",
    "for i, (logit, mask) in enumerate(zip(all_pred_logits, all_pred_masks)):\n",
    "    image_file_name = f\"{str(i).zfill(12)}.jpg\"  # Adjust this if necessary based on actual filenames\n",
    "    image_id = image_id_map.get(image_file_name)\n",
    "\n",
    "    if image_id is None:\n",
    "        print(f\"Warning: No image ID found for {image_file_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    mask_np = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    mask_rle = maskUtils.encode(np.asfortranarray(mask_np[0]))  # Assuming batch size of 1\n",
    "    mask_rle['counts'] = mask_rle['counts'].decode(\"utf-8\")\n",
    "\n",
    "    coco_results.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"category_id\": int(torch.argmax(logit).item()),\n",
    "        \"segmentation\": mask_rle,\n",
    "        \"score\": float(torch.max(logit).item())\n",
    "    })\n",
    "\n",
    "# Save and Evaluate\n",
    "if coco_results:\n",
    "    with open(\"predictions.json\", \"w\") as f:\n",
    "        json.dump(coco_results, f)\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(\"predictions.json\")\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='segm')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "else:\n",
    "    print(\"Error: coco_results is empty. No predictions available for COCO evaluation.\")\n",
    "\n",
    "# Panoptic Quality Evaluation\n",
    "pq_results = pq_compute(\n",
    "    gt_json_file=val_panoptic_file,\n",
    "    pred_json_file=\"predictions.json\",\n",
    "    gt_folder=\"datasets/coco/panoptic_val2017\",\n",
    "    pred_folder=pred_folder\n",
    ")\n",
    "\n",
    "print(f\"Panoptic Quality (PQ): {pq_results['All']['pq']}\")\n",
    "print(f\"Segmentation Quality (SQ): {pq_results['All']['sq']}\")\n",
    "print(f\"Recognition Quality (RQ): {pq_results['All']['rq']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU (mIoU): 3.1534666049992666e-05\n",
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Warning: No image ID found for YOUR_IMAGE_FILE_NAME. Skipping.\n",
      "Error: coco_results is empty. No predictions available for COCO evaluation.\n",
      "Evaluation panoptic segmentation metrics:\n",
      "Ground truth:\n",
      "\tSegmentation folder: datasets/coco/panoptic_val2017\n",
      "\tJSON file: datasets/coco/annotations/panoptic_val2017.json\n",
      "Prediction:\n",
      "\tSegmentation folder: predicted_masks_folder\n",
      "\tJSON file: predictions.json\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: coco_results is empty. No predictions available for COCO evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Panoptic Quality Evaluation\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m pq_results \u001b[38;5;241m=\u001b[39m \u001b[43mpq_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_json_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_panoptic_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Replace with your ground truth JSON file path\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_json_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredictions.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure this JSON file has the prediction results in the required format\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/coco/panoptic_val2017\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path to your ground truth mask folder\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_folder\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path to the folder with predicted masks\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPanoptic Quality (PQ): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpq_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentation Quality (SQ): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpq_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/panopticapi/evaluation.py:211\u001b[0m, in \u001b[0;36mpq_compute\u001b[0;34m(gt_json_file, pred_json_file, gt_folder, pred_folder)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pred_folder):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFolder \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with predicted segmentations doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pred_folder))\n\u001b[0;32m--> 211\u001b[0m pred_annotations \u001b[38;5;241m=\u001b[39m {el[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: el \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpred_json\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mannotations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m}\n\u001b[1;32m    212\u001b[0m matched_annotations_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gt_ann \u001b[38;5;129;01min\u001b[39;00m gt_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Step 11: Calculate metrics after running on the dataset\n",
    "miou = miou_metric.compute()\n",
    "print(f\"Mean IoU (mIoU): {miou}\")\n",
    "\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as maskUtils\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Paths to COCO files\n",
    "train_instance_file = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "val_instance_file = \"datasets/coco/annotations/instances_val2017.json\"\n",
    "val_panoptic_file = \"datasets/coco/annotations/panoptic_val2017.json\"\n",
    "\n",
    "# Load the COCO ground truth data\n",
    "coco_gt = COCO(val_instance_file)\n",
    "\n",
    "# Dictionary to map image filenames to COCO image IDs from the validation set\n",
    "with open(val_panoptic_file, \"r\") as f:\n",
    "    gt_data = json.load(f)\n",
    "image_id_map = {img['file_name']: img['id'] for img in gt_data['images']}\n",
    "\n",
    "coco_results = []\n",
    "for i, (logit, mask) in enumerate(zip(all_pred_logits, all_pred_masks)):\n",
    "    # Obtain the image file name for this prediction (adjust as per your data loader setup)\n",
    "    image_file_name = \"YOUR_IMAGE_FILE_NAME\"  # Replace with the method to get the actual image filename\n",
    "    image_id = image_id_map.get(image_file_name)\n",
    "\n",
    "    if image_id is None:\n",
    "        print(f\"Warning: No image ID found for {image_file_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Detach and prepare mask for RLE encoding\n",
    "    mask = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    mask_rle = maskUtils.encode(np.asfortranarray(mask))\n",
    "    mask_rle['counts'] = mask_rle['counts'].decode(\"utf-8\")  # JSON-compatible format\n",
    "\n",
    "    # Append prediction to results\n",
    "    coco_results.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"category_id\": int(torch.argmax(logit).item()),\n",
    "        \"segmentation\": mask_rle,\n",
    "        \"score\": float(torch.max(logit).item())\n",
    "    })\n",
    "\n",
    "# Perform COCO evaluation only if there are results\n",
    "if coco_results:\n",
    "    # Save predictions to a JSON file if necessary\n",
    "    with open(\"predictions.json\", \"w\") as f:\n",
    "        json.dump(coco_results, f)\n",
    "\n",
    "    # Load predictions into COCO format for evaluation\n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='segm')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "else:\n",
    "    print(\"Error: coco_results is empty. No predictions available for COCO evaluation.\")\n",
    "\n",
    "# Panoptic Quality Evaluation\n",
    "pq_results = pq_compute(\n",
    "    gt_json_file=val_panoptic_file,  # Replace with your ground truth JSON file path\n",
    "    pred_json_file=\"predictions.json\",  # Ensure this JSON file has the prediction results in the required format\n",
    "    gt_folder=\"datasets/coco/panoptic_val2017\",  # Path to your ground truth mask folder\n",
    "    pred_folder=pred_folder  # Path to the folder with predicted masks\n",
    ")\n",
    "\n",
    "print(f\"Panoptic Quality (PQ): {pq_results['All']['pq']}\")\n",
    "print(f\"Segmentation Quality (SQ): {pq_results['All']['sq']}\")\n",
    "print(f\"Recognition Quality (RQ): {pq_results['All']['rq']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://images.cocodataset.org/zips/train2017.zip\n",
    "http://images.cocodataset.org/zips/val2017.zip\n",
    "http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip\n",
    "http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
