{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from coco_dataset import COCOPanopticDataset\n",
    "from load_data import train_loader\n",
    "from pixeldecoder import PixelDecoder\n",
    "from backbone import BackboneWithMultiScaleFeatures\n",
    "from tokenizer import TaskTokenizer\n",
    "from mlp import TaskMLP\n",
    "from text_mapper import TextMapper\n",
    "from contrastive_loss import ContrastiveLoss\n",
    "from query_formulation import TaskConditionedQueryFormulator\n",
    "from compute_loss import SetCriterion\n",
    "from hungarian_matcher import HungarianMatcher\n",
    "from transformer_decoder import TransformerDecoder\n",
    "from predict import MaskClassPredictor\n",
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = 30000\n",
    "embed_dim = 256\n",
    "max_seq_len = 128\n",
    "num_queries = 100\n",
    "temperature = 0.2\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_classes = 80\n",
    "contrastive_weight = 0.5\n",
    "primary_loss_weight = 1.0\n",
    "\n",
    "# COCO Dataset Paths\n",
    "train_image_dir = \"datasets/coco/train2017\"\n",
    "train_instance_file = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "train_panoptic_file = \"datasets/coco/annotations/panoptic_train2017.json\"\n",
    "train_panoptic_mask_dir = \"datasets/coco/panoptic_train2017\"\n",
    "\n",
    "# Define transformation for images and masks\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize the dataset and DataLoader\n",
    "full_train_dataset = COCOPanopticDataset(\n",
    "    image_dir=train_image_dir,\n",
    "    instance_file=train_instance_file,\n",
    "    panoptic_file=train_panoptic_file,\n",
    "    panoptic_mask_dir=train_panoptic_mask_dir,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_train_dataset, range(5000))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panoptic mask 286903.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 137451.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 413734.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 251920.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 243134.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 289899.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 380140.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 277440.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 361351.png not found in datasets/coco/panoptic_train2017\n",
      "Input shape before reshaping: torch.Size([3, 1, 256])\n",
      "Shape after flattening for MLP: torch.Size([3, 256])\n",
      "Shape after MLP processing: torch.Size([3, 256])\n",
      "Shape after reshaping back to [batch_size, seq_len, output_dim]: torch.Size([3, 1, 256])\n",
      "Input shape before reshaping: torch.Size([3, 1, 256])\n",
      "Shape after flattening for MLP: torch.Size([3, 256])\n",
      "Shape after MLP processing: torch.Size([3, 256])\n",
      "Shape after reshaping back to [batch_size, seq_len, output_dim]: torch.Size([3, 1, 256])\n",
      "------------------------------------Shape of q_task: torch.Size([3, 100, 256])\n",
      "------------------------------------Shape of q_text: torch.Size([1, 3, 256])\n",
      "Contrastive Loss: 5.650376319885254\n",
      "Initial shape of q_task before permute: torch.Size([300, 256])\n",
      "q_task reshaped to add batch dimension: torch.Size([1, 300, 256])\n",
      "Shape of q_task after permute for cross-attention: torch.Size([300, 1, 256])\n",
      "Shape of k_v_1_32 for cross-attention: torch.Size([2048, 1, 256])\n",
      "Shape of k_v_1_16 for self-attention: torch.Size([4096, 1, 256])\n",
      "Shape of q_task before feed-forward network: torch.Size([1, 300, 256])\n",
      "Shape of decoder_output: torch.Size([1, 300, 256])\n",
      "Shape of image_features_1_4 before view: torch.Size([1, 256, 128, 128])\n",
      "Shape of flattened_image_features_1_4 after view and permute: torch.Size([1, 16384, 256])\n",
      "Shape of combined_input after concatenation: torch.Size([1, 16684, 256])\n",
      "src_logits shape after ensuring 2D: torch.Size([1, 1])\n",
      "target_classes shape after flattening: torch.Size([512])\n",
      "Adjusting target_classes shape to match src_logits shape.\n",
      "src_logits values:\n",
      "tensor([[-0.0585]], grad_fn=<UnsqueezeBackward0>)\n",
      "target_classes values:\n",
      "tensor([0])\n",
      "Unique values in target_classes: tensor([0])\n",
      "src_logits mean: -0.05845490097999573, std: nan\n",
      "Cross-entropy loss: 0.0\n",
      "No masks found in targets; skipping mask loss calculation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihyopark/Desktop/Reimplementation_OneFormer/compute_loss.py:46: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  print(f\"src_logits mean: {src_logits.mean()}, std: {src_logits.std()}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Loss: 5.650376319885254, Primary Loss: 0.0, Total Loss: 2.825188159942627\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model Components\n",
    "backbone = BackboneWithMultiScaleFeatures()\n",
    "pixel_decoder = PixelDecoder(input_channels=[256, 512, 1024, 2048])\n",
    "tokenizer = TaskTokenizer(vocab_size, embed_dim, max_seq_len)\n",
    "mlp = TaskMLP(input_dim=embed_dim, hidden_dim=embed_dim, output_dim=embed_dim)\n",
    "text_mapper = TextMapper(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "contrastive_loss_fn = ContrastiveLoss(temperature)\n",
    "task_query_formulator = TaskConditionedQueryFormulator(num_queries=num_queries, embed_dim=embed_dim)\n",
    "matcher = HungarianMatcher(cost_class=1, cost_mask=1, cost_dice=1)\n",
    "criterion = SetCriterion(matcher=matcher, num_classes=num_classes, weight_dict={'loss_ce': 1, 'loss_mask': 1, 'loss_dice': 1}, eos_coef=0.1, losses=['labels', 'masks'])\n",
    "transformer_decoder = TransformerDecoder(\n",
    "    embed_dim=embed_dim,\n",
    "    num_queries=num_queries,\n",
    "    num_classes=num_classes,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "mask_class_predictor = MaskClassPredictor(embed_dim, num_queries, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": backbone.parameters()},\n",
    "    {\"params\": pixel_decoder.parameters()},\n",
    "    {\"params\": transformer_decoder.parameters()},\n",
    "    {\"params\": mask_class_predictor.parameters()},\n",
    "    {\"params\": mlp.parameters()},\n",
    "    #{\"params\": tokenizer.parameters()},\n",
    "    {\"params\": text_mapper.parameters()},\n",
    "    {\"params\": task_query_formulator.parameters()},\n",
    "], lr=1e-4)\n",
    "\n",
    "# Main Training Loop (Single Batch for Debugging)\n",
    "for image_batch, mask_batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step 1: Extract Multi-Scale Features\n",
    "    multi_scale_features = backbone(image_batch)\n",
    "    decoded_features = pixel_decoder(multi_scale_features)\n",
    "    image_features_1_4 = decoded_features[0]\n",
    "    \n",
    "    # Step 2: Tokenize Task Texts\n",
    "    task_texts = [\"panoptic\", \"instance\", \"semantic\"]\n",
    "    task_embeddings = tokenizer.forward(task_texts)  # [3, max_seq_len, embed_dim]\n",
    "    task_embeddings = mlp(task_embeddings.mean(dim=1).unsqueeze(1)).squeeze(1)  # [3, embed_dim]\n",
    "    \n",
    "    # Step 3: Map Task Embeddings to Q_text\n",
    "    q_text = text_mapper(\n",
    "        panoptic_text=task_embeddings[0].unsqueeze(0).long(),\n",
    "        instance_text=task_embeddings[1].unsqueeze(0).long(),\n",
    "        semantic_text=task_embeddings[2].unsqueeze(0).long()\n",
    "    )\n",
    "\n",
    "    # Step 4: Generate Q_task\n",
    "    batch_size = image_batch.size(0)\n",
    "    q_task = task_query_formulator(task_embeddings.unsqueeze(1), batch_size).permute(1, 0, 2)\n",
    "    print(f\"------------------------------------Shape of q_task: {q_task.shape}\")\n",
    "    print(f\"------------------------------------Shape of q_text: {q_text.shape}\")\n",
    "\n",
    "    # Step 5: Calculate Contrastive Loss between Q_text and Q_task\n",
    "    contrastive_loss = contrastive_loss_fn(q_text, q_task)\n",
    "    print(f\"Contrastive Loss: {contrastive_loss.item()}\")\n",
    "    \n",
    "    ############# THIS IS FROM CONTRASTIVE_LOSS #####################\n",
    "    # Expand q_text to match q_task's batch size if necessary\n",
    "    if q_text.size(0) == 1:\n",
    "        q_text = q_text.expand(q_task.size(0), -1, -1)  # Adjust q_text to [batch_size, num_tasks, embed_dim]\n",
    "\n",
    "    # Normalize embeddings\n",
    "    q_text = F.normalize(q_text, dim=-1)\n",
    "    q_task = F.normalize(q_task, dim=-1)\n",
    "\n",
    "    batch_size, num_tasks, embed_dim = q_text.size()\n",
    "    _, num_queries, _ = q_task.size()\n",
    "    \n",
    "    # Reshape for pairwise comparison\n",
    "    q_text = q_text.reshape(batch_size * num_tasks, embed_dim)\n",
    "    q_task = q_task.reshape(batch_size * num_queries, embed_dim)\n",
    "    #################################################################\n",
    "\n",
    "    # Step 6: Integrate Image Features\n",
    "    decoder_output = transformer_decoder(q_task, multi_scale_features)\n",
    "\n",
    "    # Print shapes before concatenation\n",
    "    print(f\"Shape of decoder_output: {decoder_output.shape}\")\n",
    "    print(f\"Shape of image_features_1_4 before view: {image_features_1_4.shape}\")\n",
    "\n",
    "    # Reshape image_features_1_4 and print again\n",
    "    # Flatten the spatial dimensions (128 x 128) into a single dimension\n",
    "    # Flatten the spatial dimensions (128 x 128) into a single dimension of 16384\n",
    "    flattened_image_features_1_4 = image_features_1_4.view(1, embed_dim, 128 * 128).permute(0, 2, 1)\n",
    "    print(f\"Shape of flattened_image_features_1_4 after view and permute: {flattened_image_features_1_4.shape}\")\n",
    "\n",
    "    # Concatenate along the sequence dimension\n",
    "    combined_input = torch.cat([decoder_output, flattened_image_features_1_4], dim=1)\n",
    "    print(f\"Shape of combined_input after concatenation: {combined_input.shape}\")\n",
    "    \n",
    "    # Step 8: Mask and Class Prediction\n",
    "    mask_pred, class_pred = mask_class_predictor(combined_input)\n",
    "    \n",
    "    # Step 9: Calculate Primary Loss\n",
    "    outputs = {'pred_logits': class_pred, 'pred_masks': mask_pred}\n",
    "    targets = [{'labels': mask_batch[0]}]\n",
    "    primary_loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Combined Loss\n",
    "    total_loss = contrastive_weight * contrastive_loss + primary_loss_weight * sum(primary_loss.values())\n",
    "    \n",
    "    # Step 10: Backpropagation\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Contrastive Loss: {contrastive_loss.item()}, Primary Loss: {sum(primary_loss.values()).item()}, Total Loss: {total_loss.item()}\")\n",
    "    \n",
    "    # Break after one batch for debugging\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
