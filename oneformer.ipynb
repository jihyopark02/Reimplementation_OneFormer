{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reimplementing OneFormer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from coco_dataset import COCOPanopticDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Resize images to 512x512\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize based on ImageNet means and std\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Paths to COCO data\n",
    "train_image_dir = \"datasets/coco/train2017\"\n",
    "train_instance_file = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "train_panoptic_file = \"datasets/coco/annotations/panoptic_train2017.json\"\n",
    "train_panoptic_mask_dir = \"datasets/coco/panoptic_train2017\"\n",
    "\n",
    "# Initialize dataset\n",
    "train_dataset = COCOPanopticDataset(\n",
    "    image_dir=train_image_dir,\n",
    "    instance_file=train_instance_file,\n",
    "    panoptic_file=train_panoptic_file,\n",
    "    panoptic_mask_dir=train_panoptic_mask_dir,\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panoptic mask 470036.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 10407.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 552054.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 324937.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 192817.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 162087.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 354644.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 326063.png not found in datasets/coco/panoptic_train2017\n",
      "Batch of images shape: torch.Size([2, 3, 512, 512])\n",
      "Batch of masks shape: torch.Size([2, 512, 512])\n",
      "Panoptic mask 464265.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 290314.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 286925.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 425158.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 14108.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 216899.png not found in datasets/coco/panoptic_train2017\n"
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(\"Batch of images shape:\", images.shape)  # Expected shape: [batch_size, 3, 512, 512]\n",
    "    print(\"Batch of masks shape:\", masks.shape)    # Expected shape: [batch_size, 512, 512]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panoptic mask 131856.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 118690.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 181906.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 433968.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 479528.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 153671.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 9813.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 342624.png not found in datasets/coco/panoptic_train2017\n",
      "Batch 1\n",
      "Batch of images shape: torch.Size([2, 3, 512, 512])\n",
      "Batch of masks shape: torch.Size([2, 512, 512])\n",
      "Panoptic mask 473102.png not found in datasets/coco/panoptic_train2017\n",
      "Batch 2\n",
      "Batch of images shape: torch.Size([2, 3, 512, 512])\n",
      "Batch of masks shape: torch.Size([2, 512, 512])\n",
      "Panoptic mask 202931.png not found in datasets/coco/panoptic_train2017\n",
      "Batch 3\n",
      "Batch of images shape: torch.Size([2, 3, 512, 512])\n",
      "Batch of masks shape: torch.Size([2, 512, 512])\n",
      "Panoptic mask 325385.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 220988.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 511425.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 27562.png not found in datasets/coco/panoptic_train2017\n"
     ]
    }
   ],
   "source": [
    "for i, (images, masks) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(\"Batch of images shape:\", images.shape)  # Expected shape: [batch_size, 3, 512, 512]\n",
    "    print(\"Batch of masks shape:\", masks.shape)    # Expected shape: [batch_size, 512, 512]\n",
    "\n",
    "    # Stop after 3 batches\n",
    "    if i == 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PixelDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super(PixelDecoder, self).__init__()\n",
    "        # Define layers for each scale with matching input channels\n",
    "        self.layer1 = nn.Conv2d(2048, embed_dim, kernel_size=3, padding=1)  # For Layer4\n",
    "        self.layer2 = nn.Conv2d(1024, embed_dim, kernel_size=3, padding=1)  # For Layer3\n",
    "        self.layer3 = nn.Conv2d(512, embed_dim, kernel_size=3, padding=1)   # For Layer2\n",
    "        self.layer4 = nn.Conv2d(256, embed_dim, kernel_size=3, padding=1)   # For Layer1\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Assuming `features` is a list of feature maps from ResNet backbone\n",
    "        multi_scale_features = []\n",
    "        for i, layer in enumerate([self.layer1, self.layer2, self.layer3, self.layer4]):\n",
    "            # Interpolate each feature map to the same size as the last feature map (smallest resolution)\n",
    "            scaled_feature = F.interpolate(features[i], size=(features[0].shape[2], features[0].shape[3]), mode='bilinear')\n",
    "            multi_scale_features.append(layer(scaled_feature))\n",
    "        \n",
    "        return multi_scale_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "from detectron2.modeling import build_backbone\n",
    "from detectron2.config import get_cfg, CfgNode\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize configuration and allow setting new attributes\n",
    "cfg = get_cfg()\n",
    "cfg.MODEL.ONE_FORMER = CfgNode()\n",
    "cfg.MODEL.ONE_FORMER.set_new_allowed(True)\n",
    "\n",
    "# General settings for ONE_FORMER\n",
    "cfg.MODEL.ONE_FORMER.NUM_OBJECT_QUERIES = 100  # Number of queries\n",
    "cfg.MODEL.ONE_FORMER.DEEP_SUPERVISION = True   # Enable/disable deep supervision\n",
    "cfg.MODEL.ONE_FORMER.NO_OBJECT_WEIGHT = 0.1    # Weight for no-object class\n",
    "cfg.MODEL.ONE_FORMER.CLASS_WEIGHT = 1.0        # Class weight\n",
    "cfg.MODEL.ONE_FORMER.DICE_WEIGHT = 1.0         # Dice loss weight\n",
    "cfg.MODEL.ONE_FORMER.MASK_WEIGHT = 1.0         # Mask weight\n",
    "cfg.MODEL.ONE_FORMER.CONTRASTIVE_WEIGHT = 0.07 # Contrastive weight\n",
    "\n",
    "# Training settings\n",
    "cfg.MODEL.ONE_FORMER.TRAIN_NUM_POINTS = 12544  # Number of training points\n",
    "cfg.MODEL.ONE_FORMER.OVERSAMPLE_RATIO = 3.0    # Oversample ratio\n",
    "cfg.MODEL.ONE_FORMER.IMPORTANCE_SAMPLE_RATIO = 0.75  # Importance sample ratio\n",
    "cfg.MODEL.ONE_FORMER.SIZE_DIVISIBILITY = 32    # Size divisibility for input\n",
    "\n",
    "# TEXT_ENCODER settings\n",
    "cfg.MODEL.TEXT_ENCODER = CfgNode()\n",
    "cfg.MODEL.TEXT_ENCODER.set_new_allowed(True)\n",
    "cfg.MODEL.TEXT_ENCODER.VOCAB_SIZE = 30000      # Vocabulary size\n",
    "cfg.MODEL.TEXT_ENCODER.WIDTH = 256             # Embedding dimension\n",
    "cfg.MODEL.TEXT_ENCODER.CONTEXT_LENGTH = 128    # Context window length\n",
    "cfg.MODEL.TEXT_ENCODER.PROJ_NUM_LAYERS = 2     # Projection layers\n",
    "cfg.MODEL.TEXT_ENCODER.N_CTX = 32              # Context size\n",
    "\n",
    "# Input configuration\n",
    "cfg.INPUT = CfgNode()\n",
    "cfg.INPUT.set_new_allowed(True)\n",
    "cfg.INPUT.TASK_SEQ_LEN = 128                   # Task sequence length\n",
    "cfg.INPUT.MAX_SEQ_LEN = 512                    # Max sequence length\n",
    "\n",
    "# Pixel mean and std for normalization\n",
    "cfg.MODEL.PIXEL_MEAN = [103.530, 116.280, 123.675]\n",
    "cfg.MODEL.PIXEL_STD = [1.0, 1.0, 1.0]\n",
    "\n",
    "# Post-processing and inference settings\n",
    "cfg.MODEL.TEST = CfgNode()\n",
    "cfg.MODEL.TEST.set_new_allowed(True)\n",
    "cfg.MODEL.TEST.OBJECT_MASK_THRESHOLD = 0.5\n",
    "cfg.MODEL.TEST.OVERLAP_THRESHOLD = 0.7\n",
    "cfg.MODEL.TEST.SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE = False\n",
    "cfg.MODEL.TEST.PANOPTIC_ON = True\n",
    "cfg.MODEL.TEST.INSTANCE_ON = True\n",
    "cfg.MODEL.TEST.DETECTION_ON = False\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 100\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=8, num_layers=6):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Build backbone with Detectron2 configuration\n",
    "        self.backbone = build_backbone(cfg)\n",
    "        \n",
    "\n",
    "        # Pixel Decoder setup\n",
    "        self.pixel_decoder = PixelDecoder(embed_dim=embed_dim)\n",
    "        final_layer_channels = self.backbone.output_shape()['res4'].channels\n",
    "        self.conv1x1 = nn.Conv2d(final_layer_channels, embed_dim, kernel_size=1)\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the Detectron2 backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Check and transform final backbone feature layer for transformer encoder\n",
    "        res4_feature = features['res4']\n",
    "        res4_feature_reduced = self.conv1x1(res4_feature)\n",
    "        \n",
    "        # Transformer expects flattened features\n",
    "        B, C, H, W = res4_feature_reduced.shape\n",
    "        features_flattened = res4_feature_reduced.flatten(2).permute(2, 0, 1)  # [H*W, B, embed_dim]\n",
    "\n",
    "        # Pass through Transformer Encoder\n",
    "        encoded_features = self.transformer_encoder(features_flattened)\n",
    "        encoded_features = encoded_features.permute(1, 2, 0).view(B, C, H, W)\n",
    "        \n",
    "        return res4_feature_reduced, encoded_features\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_classes=21, num_queries=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim, num_heads=8)\n",
    "        \n",
    "        # Linear layer to map queries to class logits\n",
    "        self.query_to_mask = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Final convolution layer to map back to image dimensions\n",
    "        self.conv1 = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, features, queries):\n",
    "        B, C, H, W = features.shape\n",
    "        features_flattened = features.flatten(2).permute(2, 0, 1)  # Shape: [H*W, batch, embed_dim]\n",
    "        \n",
    "        # Cross-attention between queries and flattened features\n",
    "        attended_queries, _ = self.cross_attention(queries, features_flattened, features_flattened)\n",
    "        \n",
    "        # Map attended queries to class logits\n",
    "        mask_logits = self.query_to_mask(attended_queries)  # Shape: [num_queries, batch, num_classes]\n",
    "        \n",
    "        # Reshape features for final convolution\n",
    "        attended_features = features + features.mean(dim=[2, 3], keepdim=True)  # Add query information to features\n",
    "        x = self.conv1(attended_features)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStageDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_classes=21, num_queries=100, num_stages=3):\n",
    "        super(MultiStageDecoder, self).__init__()\n",
    "        self.num_stages = num_stages\n",
    "        self.stages = nn.ModuleList([Decoder(embed_dim, num_classes) for _ in range(num_stages)])\n",
    "\n",
    "    def forward(self, features, queries):\n",
    "        output = None\n",
    "        for stage in self.stages:\n",
    "            output = stage(features, queries)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_loss(pred_masks, gt_masks, gt_labels):\n",
    "    # Cross-entropy for semantic segmentation\n",
    "    ce_loss = F.cross_entropy(pred_masks, gt_labels, ignore_index=-1)  # Add ignore_index if needed\n",
    "\n",
    "    # Dice loss for mask overlap\n",
    "    dice = dice_loss(pred_masks, gt_masks)\n",
    "    \n",
    "    # Query-to-mask matching loss (if implemented in your OneFormer version)\n",
    "    query_loss = query_to_mask_loss(pred_masks, gt_masks, num_queries=pred_masks.shape[1])\n",
    "    \n",
    "    return ce_loss + dice + query_loss\n",
    "\n",
    "def dice_loss(pred, target, smooth=1):\n",
    "    # Flatten tensors to calculate overlap\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    \n",
    "    return 1 - dice\n",
    "\n",
    "def query_to_mask_loss(pred_masks, gt_masks, num_queries):\n",
    "    # Placeholder for query-to-mask matching loss\n",
    "    # Implement Hungarian matching if required\n",
    "    return torch.tensor(0.0)  # Temporary placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneFormer(nn.Module):\n",
    "    def __init__(self, num_classes=21, embed_dim=256, num_heads=8, num_layers=6, num_queries=100):\n",
    "        super(OneFormer, self).__init__()\n",
    "        self.encoder = Encoder(embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "        self.num_queries = num_queries\n",
    "        self.query_embed = nn.Embedding(num_queries, embed_dim)\n",
    "        self.task_embeddings = nn.Embedding(3, embed_dim)\n",
    "        self.task_mlp = MLP(cfg.INPUT.TASK_SEQ_LEN, embed_dim, embed_dim, 2)\n",
    "        self.text_encoder = nn.Embedding(cfg.MODEL.TEXT_ENCODER.VOCAB_SIZE, embed_dim)\n",
    "        self.text_projector = nn.Linear(embed_dim, embed_dim)\n",
    "        self.decoder = MultiStageDecoder(embed_dim=embed_dim, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, task_type, masks=None):\n",
    "        task_embed = self.task_embeddings(torch.tensor([task_type], device=x.device)).unsqueeze(1)\n",
    "        multi_scale_features, encoded_features = self.encoder(x)\n",
    "        B = x.shape[0]\n",
    "        queries = self.query_embed.weight.unsqueeze(1).repeat(1, B, 1) + task_embed\n",
    "        segmentation_output = self.decoder(encoded_features, queries)\n",
    "\n",
    "        print(\"Segmentation Output Shape:\", segmentation_output.shape)  # Debugging line\n",
    "        \n",
    "        if self.training:\n",
    "            # Compute and return training losses\n",
    "            if masks is not None:\n",
    "                losses = compute_loss(segmentation_output, masks['masks'], masks['labels'])\n",
    "                return losses\n",
    "            else:\n",
    "                raise ValueError(\"Missing ground truth masks for training.\")\n",
    "        else:\n",
    "            # Perform inference (use semantic, panoptic, or instance inference as needed)\n",
    "            return segmentation_output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panoptic mask 251439.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 507686.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 539397.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 473706.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 84592.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 553192.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 115752.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 445933.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 564938.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 441619.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 136299.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 503293.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 123382.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 116819.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 573750.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 459600.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 527048.png not found in datasets/coco/panoptic_train2017\n",
      "Panoptic mask 30769.png not found in datasets/coco/panoptic_train2017\n",
      "Segmentation Output Shape: torch.Size([2, 21, 32, 32])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing ground truth masks for training.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m)  \u001b[38;5;66;03m# [batch_size, channels, height, width]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     10\u001b[0m         loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(output, targets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m], targets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[88], line 28\u001b[0m, in \u001b[0;36mOneFormer.forward\u001b[0;34m(self, x, task_type, masks)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m losses\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing ground truth masks for training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Perform inference (use semantic, panoptic, or instance inference as needed)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m segmentation_output\n",
      "\u001b[0;31mValueError\u001b[0m: Missing ground truth masks for training."
     ]
    }
   ],
   "source": [
    "# Set up the model\n",
    "num_classes = 21  # Adjust this based on your specific dataset (e.g., COCO has 80 classes)\n",
    "model = OneFormer(num_classes=num_classes)\n",
    "\n",
    "# Test with a sample batch\n",
    "sample_batch = torch.randn(2, 3, 512, 512)  # [batch_size, channels, height, width]\n",
    "for images, targets in train_loader:\n",
    "    output = model(sample_batch, task_type=0)\n",
    "    if model.training:\n",
    "        loss = model.compute_loss(output, targets['masks'], targets['labels'])\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: [batch_size, num_classes, height, width]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
